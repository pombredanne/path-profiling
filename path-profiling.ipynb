{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path profiling with sketches\n",
    "\n",
    "If we want to characterize how process control moves through a graph over time, _path profiling_ is a general approach that captures detail that node or edge profiling might miss.  Depending on the concrete application of path profiling, several engineering constraints may be more or less important:  do we want to count a small number of paths quickly and precisely or a much larger number of paths scalably?\n",
    "\n",
    "The classic approach of [Ball and Larus](https://dl.acm.org/citation.cfm?id=243857) ([pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.7451&rep=rep1&type=pdf)) provides an efficient technique for tracking acyclic intraprocedural paths through the control-flow graphs (CFG) of compiled programs.  Essentially, each possible path is represented by an integer, which is constructed by setting bits on block entry.  (Their technique constructs a spanning tree of the CFG so that it doesn't have to set bits after _every_ branch.)  The path numbering then indexes into an array of counters (for functions with a small number of possible paths) or into a (precise) hash table of counters (for functions with a large number of possible paths but sparsely-distributed actual paths).\n",
    "\n",
    "In this notebook, we'll evaluate path profiling for a different problem with different constraints:  profiling the top paths to exceptional states in synthetic business process flowcharts.  The charts resemble function control-flow graphs but may have substantially more nodes than a typical compiled function and (especially since we may want to explicitly consider cycles, at least up to a certain loop count) substantially longer paths.  We also would like to maintain a separate profile for the paths leading to each terminal node, so that we can identify the most likely ways to get to a particular exceptional state and use this information to refine our processes.  Since compiled functions are typically small and the basic blocks within these may only have fewer than ten instructions, the cost of explicitly updating a counter upon entering a block (let alone building a dynamic structure to represent a path) may be prohibitive.  By contrast, the processes we're interested in modeling are made up of nodes that take longer to execute than a basic block, so we can maintain a dynamic path until a terminal node -- the problem is that we may not be able to precisely count the number of paths we've encountered.\n",
    "\n",
    "We'll handle this problem by using a sketch to track the top *k* paths to each terminal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from ipysigma import Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_flowchart(size=100):\n",
    "    import numpy as np\n",
    "    \n",
    "    def insert_step(g, f, t, n):\n",
    "        \"\"\" inserts n as an extra node between f and t; removes an existing edge between f and t \"\"\"\n",
    "        g.add_edge(f, n)\n",
    "        g.add_edge(n, t)\n",
    "        g.remove_edge(f, t)\n",
    "        \n",
    "    def insert_branch(g, f, t, n):\n",
    "        \"\"\" inserts n as an extra node between f and t \"\"\"\n",
    "        g.add_edge(f, n)\n",
    "        g.add_edge(n, t)\n",
    "    \n",
    "    def insert_leaf(g, f, n):\n",
    "        \"\"\" inserts n as a leaf node with an edge from f \"\"\"\n",
    "        g.add_edge(f, n)\n",
    "        \n",
    "    def create_loop(g, f, t):\n",
    "        \"\"\" inserts a back edge from t to f \"\"\"\n",
    "        g.add_edge(t, f)\n",
    "    \n",
    "    def choose_edge(g):\n",
    "        # mea maxima culpa\n",
    "        edges = list(g.edges())\n",
    "        return edges[np.random.randint(len(edges))]\n",
    "    \n",
    "    def choose_node(g):\n",
    "        nodes = list(g)\n",
    "        return np.random.choice(nodes)\n",
    "        \n",
    "    g = nx.DiGraph()\n",
    "    g.add_edge(0, 1)\n",
    "    \n",
    "    next_node = 2\n",
    "    while next_node <= size:\n",
    "        edges = g.edges()\n",
    "        nodes = g.nodes()\n",
    "        which = np.random.randint(100)\n",
    "        if which <= 30:\n",
    "            f, t = choose_edge(g)\n",
    "            insert_step(g, f, t, next_node)\n",
    "            next_node = next_node + 1\n",
    "        elif which <= 60:\n",
    "            f, t = choose_edge(g)\n",
    "            insert_branch(g, f, t, next_node)\n",
    "            next_node = next_node + 1\n",
    "        elif which <= 80:\n",
    "            f, t = choose_edge(g)\n",
    "            create_loop(g, f, t)\n",
    "        else:\n",
    "            insert_leaf(g, choose_node(g), next_node)\n",
    "            next_node = next_node + 1\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = synthetic_flowchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f17bc09a7e4934882ddd4af83fc4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(data={'nodes': [(0, {}), (1, {}), (2, {}), (3, {}), (4, {}), (5, {}), (6, {}), (7, {}), (8, {}), (9, {})…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Sigma(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate random paths through our random graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path(g, bound=100):\n",
    "    \"\"\" find a random path of at most size bound through a directed graph \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    path = [0]\n",
    "    plen = 1\n",
    "    while plen < bound:\n",
    "        edges = list(g.edges(path[-1]))\n",
    "        if len(edges) == 0:\n",
    "            return path\n",
    "        c, n = edges[np.random.randint(len(edges))]\n",
    "        path.append(n)\n",
    "        plen = plen + 1\n",
    "    return path\n",
    "\n",
    "def unipath(g, bound=100):\n",
    "    \"\"\" find a random path of at most size bound through a directed graph; \n",
    "        return a representation of the path as a Unicode string (so it's hashable) \"\"\"\n",
    "    p = path(g, bound)\n",
    "    return \"\".join([chr(0x1f600 + c) for c in p])\n",
    "\n",
    "def deunipath(p):\n",
    "    \"\"\" translate a unicode representation of a path into a list representation of that path \"\"\"\n",
    "    return [ord(c) - 0x1f600 for c in p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also turn each path into a string  (Storing paths as strings will be necessary so that we can hash them and count them in a sketch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'😀😆😽😐🙟😡😟'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unipath(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketching paths\n",
    "\n",
    "We'll use a count-min sketch paired with a priority queue in order to track the top ten paths over many simulations.  The actual count-min sketch implementation isn't important, but we'll include it here in case you want to dig in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "def hashes_for(count, stride):\n",
    "    def hashes(value):\n",
    "        bvalue = type(value) == bytes and value or pickle.dumps(value)\n",
    "        digest = sha1(bvalue).hexdigest()\n",
    "        return [int(digest[s:s+stride], 16) for s in [x * stride for x in range(count)]]\n",
    "    return hashes\n",
    "\n",
    "class CMS(object):\n",
    "    def __init__(self, width, hashes):\n",
    "        \"\"\" Initializes a Count-min sketch with the\n",
    "            given width and a collection of hashes, \n",
    "            which are functions taking arbitrary \n",
    "            values and returning integers.  The depth\n",
    "            of the sketch structure is taken from the\n",
    "            number of supplied hash functions.\n",
    "            \n",
    "            hashes can be either a function taking \n",
    "            a value and returning a list of results\n",
    "            or a list of functions.  In the latter \n",
    "            case, this constructor will synthesize \n",
    "            the former \"\"\"\n",
    "        self.__width = width\n",
    "        \n",
    "        if hasattr(hashes, '__call__'):\n",
    "            self.__hashes = hashes\n",
    "            # inspect the tuple returned by the hash function to get a depth\n",
    "            self.__depth = len(hashes(bytes()))\n",
    "        else:\n",
    "            funs = hashes[:]\n",
    "            self.__depth = len(hashes)\n",
    "            def h(value):\n",
    "                return [int(f(value)) for f in funs]\n",
    "            self.__hashes = h\n",
    "        \n",
    "        self.__buckets = numpy.zeros((int(width), int(self.__depth)), numpy.uint64)\n",
    "    \n",
    "    \n",
    "    def width(self):\n",
    "        return self.__width\n",
    "    \n",
    "    def depth(self):\n",
    "        return self.__depth\n",
    "    \n",
    "    def insert(self, value):\n",
    "        \"\"\" Inserts a value into this sketch \"\"\"\n",
    "        for (row, col) in enumerate(self.__hashes(value)):\n",
    "            self.__buckets[col % self.__width][row] += 1\n",
    "    def lookup(self, value):\n",
    "        \"\"\" Returns a biased estimate of number of times value has been inserted in this sketch\"\"\"\n",
    "        return min([self.__buckets[col % self.__width][row] for (row, col) in enumerate(self.__hashes(value))])\n",
    "    \n",
    "    def merge_from(self, other):\n",
    "        \"\"\" Merges other in to this sketch by \n",
    "            adding the counts from each bucket in other\n",
    "            to the corresponding buckets in this\n",
    "            \n",
    "            Updates this. \"\"\"\n",
    "        self.__buckets += other.__buckets\n",
    "    \n",
    "    def merge(self, other):\n",
    "        \"\"\" Creates a new sketch by merging this sketch's\n",
    "            counts with those of another sketch. \"\"\"\n",
    "        \n",
    "        cms = CMS(self.width(), self.__hashes)\n",
    "        cms.__buckets += self.__buckets\n",
    "        cms.__buckets += other.__buckets\n",
    "        return cms\n",
    "\n",
    "    def dup(self):\n",
    "        cms = CMS(self.width(), self.__hashes)\n",
    "        cms.merge_from(self)\n",
    "        return cms\n",
    "\n",
    "class TopK(object):\n",
    "    import heapq\n",
    "    def __init__(self, k, cms=None):\n",
    "        if cms is None:\n",
    "            self._cms = CMS(32768, hashes_for(3,8))\n",
    "        else:\n",
    "            self._cms = cms.dup()\n",
    "        self.pq = []\n",
    "        self.topset = set()\n",
    "        self.k = k\n",
    "    \n",
    "    def put(self, obj, f=lambda x: x):\n",
    "        self._cms.insert(f(obj))\n",
    "        count = self._cms.lookup(f(obj))\n",
    "        update = (count, obj)\n",
    "        if obj in self.topset:\n",
    "            self.pq = [(o == obj and count or c, o) for (c, o) in self.pq]\n",
    "            heapq.heapify(self.pq)\n",
    "        else:\n",
    "            self.topset.add(obj)\n",
    "            if len(self.topset) > self.k:\n",
    "                (c, remove) = heapq.heappushpop(self.pq, update)\n",
    "                self.topset.remove(remove)\n",
    "            else:\n",
    "                heapq.heappush(self.pq, update)\n",
    "    \n",
    "    def top(self):\n",
    "        tk = self.pq.copy()\n",
    "        tk.sort()\n",
    "        tk.reverse()\n",
    "        return tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "def simulate(g, trials=100000, topk=10):\n",
    "    pq = []\n",
    "    topset = set()\n",
    "    cms = CMS(16384, hashes_for(3,8))\n",
    "    for i in range(trials):\n",
    "        path = unipath(g)\n",
    "        cms.insert(path.encode())\n",
    "        count = cms.lookup(path.encode())\n",
    "        update = (count, path)\n",
    "        if path in topset:\n",
    "            pq = [(p == path and count or c, p) for (c, p) in pq]\n",
    "            heapq.heapify(pq)\n",
    "        else:\n",
    "            topset.add(path)\n",
    "            if len(topset) > topk:\n",
    "                (c, remove) = heapq.heappushpop(pq, update)\n",
    "                topset.remove(remove)\n",
    "            else:\n",
    "                heapq.heappush(pq, update)\n",
    "    pq.sort()\n",
    "    pq.reverse()\n",
    "    return pq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4151, '😀😊🙃🙞'),\n",
       " (4105, '😀😊😼😛😷😔'),\n",
       " (2808, '😀😬😱😧🙎'),\n",
       " (2770, '😀😬😱🙌😧🙎'),\n",
       " (2725, '😀😬😱🙔🙐🙌😧🙎'),\n",
       " (2155, '😀😊🙃😛😷😔'),\n",
       " (1503, '😀😶😬😱😧🙎'),\n",
       " (1436, '😀😶🙋😬😱🙌😧🙎'),\n",
       " (1419, '😀😶😬😱🙔🙐🙌😧🙎'),\n",
       " (1388, '😀😶🙋😬😱🙔🙐🙌😧🙎')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate(g, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up a different simulation, tracking the top paths to each terminal node in a path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_terminals(g, trials=100000, topk=10):\n",
    "    paths_for_terminals = {}\n",
    "    for i in range(trials):\n",
    "        path = unipath(g)\n",
    "        terminal = path[-1]\n",
    "        if terminal not in paths_for_terminals:\n",
    "            paths_for_terminals[terminal] = TopK(topk)\n",
    "        paths_for_terminals[terminal].put(path, lambda p: p.encode())\n",
    "    return paths_for_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = simulate_terminals(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "😲\n",
      "   6 😀😊😗😒😚😄😏😂🙑😸😎🙈😁😑😇😋😲\n",
      "   4 😀😊😗😒😚😄😏😂😸😎😁😑😇😋😲\n",
      "   3 😀😊😗😒😚😄😏😂😸😎🙈😁😑😇😋😲\n",
      "   3 😀😊😗😒😚😄😈😂🙑😸😎😁🙕😑😇😋😲\n",
      "   2 😀😶🙋😬😱🙌😧😭🙓😅😄🙢😈🙉😂😸😎😁😑😇😋😲\n",
      "   2 😀😬😱🙔🙐🙌😧😭🙓😅😢🙂🙅😩🙘😄😞😥🙒😕😀😶🙋😬😱🙔🙐🙌😧🙌😧🙌😧😭🙓😅😄😞😕😀😊😗😒😚😄😏😂🙑😸😂😯😌😂😸😎🙈😁😑😇😋😲\n",
      "   2 😀😬😱🙌😧😭🙓😅😄😏😂🙝🙜🙀😂🙑😸😎😁😑😇😋😲\n",
      "   2 😀😬😱🙌😧😭😅😢😩🙘😄😺😏😂🙑😸😎😁😑😇😋😲\n",
      "   2 😀😬😱😧😭🙓😅😢😄😈😵😰😌😂😌😂😈🙉😂😸😂😈😵😰😌😯😌😯😌😯😌😂😸😎🙈😁🙕😑😇😋😲\n",
      "   2 😀😬😱😧😭😅😢🙂🙅🙚😩😄🙢😈🙉😂🙑😸😂🙑😸😎😁🙕😑😇😋😜😃😪😦😁😑😇😋😇😋😜😃😪😦😹😁😑😇😋😇😋😜😃😪😦😁😑😇😋😲\n",
      "\n",
      "🙎\n",
      "   2842 😀😬😱🙌😧🙎\n",
      "   2765 😀😬😱😧🙎\n",
      "   2737 😀😬😱🙔🙐🙌😧🙎\n",
      "   1444 😀😶😬😱😧🙎\n",
      "   1428 😀😶😬😱🙌😧🙎\n",
      "   1425 😀😶😬😱🙔🙐🙌😧🙎\n",
      "   1420 😀😶🙋😬😱🙔🙐🙌😧🙎\n",
      "   1391 😀😶🙋😬😱🙌😧🙎\n",
      "   1357 😀😶🙋😬😱😧🙎\n",
      "   907 😀😬😱🙌😧🙌😧🙎\n",
      "\n",
      "🙁\n",
      "   461 😀😬😱😧😭🙓😅🙁\n",
      "   456 😀😬😱🙔🙐🙌😧😭🙓😅🙁\n",
      "   453 😀😬😱🙌😧😭🙓😅🙁\n",
      "   448 😀😬😱🙌😧😭😅🙁\n",
      "   445 😀😬😱🙔🙐🙌😧😭😅🙁\n",
      "   423 😀😬😱😧😭😅🙁\n",
      "   270 😀😶😬😱🙌😧😭😅🙁\n",
      "   256 😀😊😗😒😚😄😅🙁\n",
      "   253 😀😶🙋😬😱🙔🙐🙌😧😭🙓😅🙁\n",
      "   243 😀😶🙋😬😱😧😭🙓😅🙁\n",
      "\n",
      "😟\n",
      "   1086 😀😆🙤🙄😽😐🙟😡😟\n",
      "   1086 😀😆😐🙟😡😟\n",
      "   1083 😀😆😴😐🙟😡😟\n",
      "   1081 😀😆😽😐🙟😡😟\n",
      "   1043 😀😆🙄😽😐🙟😡😟\n",
      "   1008 😀😆🙇🙄😽😐🙟😡😟\n",
      "   290 😀😆🙇🙄😽😐😙😐🙟😡😟\n",
      "   273 😀😆😐😙😐🙟😡😟\n",
      "   267 😀😆🙤🙄😽😐😙😐🙟😡😟\n",
      "   267 😀😆🙄😽😐😙😐🙟😡😟\n",
      "\n",
      "😨\n",
      "   779 😀😊😗😒😚😄😨\n",
      "   138 😀😊😼😛🙡😊😗😒😚😄😨\n",
      "   115 😀😆🙤🙄😽😐😫😄😨\n",
      "   114 😀😆🙇🙄😽😐🙍😄😨\n",
      "   114 😀😆😽😐😫😄😨\n",
      "   113 😀😆🙤🙄😽😐🙍😄😨\n",
      "   113 😀😆😴😐😫😄😨\n",
      "   109 😀😆🙇🙄😽😐😫😄😨\n",
      "   107 😀😆😽😐🙍😄😨\n",
      "   106 😀😆😴😐🙍😄😨\n",
      "\n",
      "🙠\n",
      "   305 😀😊😗😒😚😄😺🙠\n",
      "   56 😀😊😼😛🙡😊😗😒😚😄😺🙠\n",
      "   46 😀😆😽😐🙍😄😺🙠\n",
      "   43 😀😆🙇🙄😽😐🙍😄😺🙠\n",
      "   40 😀😆😽😐😫😄😺🙠\n",
      "   38 😀😆🙤🙄😽😐🙍😄😺🙠\n",
      "   38 😀😆😐😫😄😺🙠\n",
      "   36 😀😆🙤🙄😽😐😫😄😺🙠\n",
      "   34 😀😆🙄😽😐😫😄😺🙠\n",
      "   34 😀😆😐🙍😄😺🙠\n",
      "\n",
      "😔\n",
      "   4119 😀😊😼😛😷😔\n",
      "   1969 😀😊🙃😛😷😔\n",
      "   708 😀😊😼😛🙡😊😼😛😷😔\n",
      "   360 😀😊🙃😛🙡😊😼😛😷😔\n",
      "   348 😀😊😼😛🙡😊🙃😛😷😔\n",
      "   170 😀😊🙃😛🙡😊🙃😛😷😔\n",
      "   94 😀😊😼😛🙡😊😼😛🙡😊😼😛😷😔\n",
      "   60 😀😊😼😛🙡😊🙃😛🙡😊😼😛😷😔\n",
      "   58 😀😊😼😛🙡😊😼😛🙡😊🙃😛😷😔\n",
      "   56 😀😊🙃😛🙡😊😼😛🙡😊😼😛😷😔\n",
      "\n",
      "🙗\n",
      "   6 😀😊😗😒😚😄😏😂🙑😸😎😁🙕🙗\n",
      "   5 😀😊😗😒😚😄😏😂🙑😸😎🙈😁🙕🙗\n",
      "   4 😀😊😗😒😚😄😏😂😸😎😁🙕🙗\n",
      "   3 😀😬😱🙔🙐🙌😧😭🙓😅😢😄😏😂😸😎😁🙕🙗\n",
      "   3 😀😊😗😒😚😄😏🙊😏😂🙑😸😎🙈😁🙕🙗\n",
      "   3 😀😊😗😒😚😄😏😂🙝🙣🙜🙀😂😸😎🙈😁🙕🙗\n",
      "   3 😀😊😗😒😚😄😏😂😸😎🙈😁🙕🙗\n",
      "   3 😀😊😗😒😚😄😈😂😸😎😁🙕🙗\n",
      "   2 😀😶🙋😬😱🙌😧😭😅😄😏😂😸😎🙈😁🙕🙗\n",
      "   2 😀😶😬😱😧🙌😧😭🙓😅😄😏😂🙑😸😎😁🙕🙗\n",
      "\n",
      "😘\n",
      "   35 😀😊😗😒😚😄😈😍😉😖🙙🙛😘\n",
      "   33 😀😊😗😒😚😄🙢😈😍😉😖🙙🙛😘\n",
      "   24 😀😊😗😒😚😄😈😍😉😖😾😮😘\n",
      "   17 😀😊😗😒😚😄🙢😈😍😉😖😾😮🙆😘\n",
      "   15 😀😊😗😒😚😄🙢😈😍😉😖😾😮😘\n",
      "   14 😀😊😗😒😚😄😈😍😉😖😾😮🙆😘\n",
      "   13 😀😊😼😛🙡😊😗😒😚😄🙢😈😍😉😖🙙🙛😘\n",
      "   8 😀😆🙄😽😐🙍😄🙢😈😍😉😖🙙🙛😘\n",
      "   7 😀😊😼😛🙡😊😗😒😚😄😈😍😉😖😾😮🙆😘\n",
      "   6 😀😊😼😛🙡😊😗😒😚😄😈😍😉😖🙙🙛😘\n",
      "\n",
      "🙞\n",
      "   4184 😀😊🙃🙞\n",
      "   669 😀😊😼😛🙡😊🙃🙞\n",
      "   350 😀😊🙃😛🙡😊🙃🙞\n",
      "   111 😀😊😼😛🙡😊😼😛🙡😊🙃🙞\n",
      "   66 😀😊😼😛🙡😊🙃😛🙡😊🙃🙞\n",
      "   38 😀😊🙃😛🙡😊😼😛🙡😊🙃🙞\n",
      "   37 😀😊😗😒😚😄😕😀😊🙃🙞\n",
      "   36 😀😊😗😒😚😄😀😊🙃🙞\n",
      "   23 😀😊😼😛🙡😊😼😛🙡😊😼😛🙡😊🙃🙞\n",
      "   21 😀😊🙃😛🙡😊🙃😛🙡😊🙃🙞\n",
      "\n",
      "😿\n",
      "   58 😀😊😗😒😚😄😏😂😿\n",
      "   33 😀😊😗😒😚😄🙢😈😂😿\n",
      "   28 😀😊😗😒😚😄😏🙊😝🙏🙀😂😿\n",
      "   22 😀😊😗😒😚😄😺😏😂😿\n",
      "   22 😀😊😗😒😚😄😈😂😿\n",
      "   19 😀😊😗😒😚😄🙢😈🙉😂😿\n",
      "   18 😀😊😗😒😚😄😈🙉😂😿\n",
      "   15 😀😊😗😒😚😄😏🙊😝😂😿\n",
      "   15 😀😊😗😒😚😄😏🙊😏😂😿\n",
      "   15 😀😊😗😒😚😄😈😠😌😂😿\n",
      "\n",
      "😻\n",
      "   23 😀😊😗😒😚😄🙢😈😍😉😓😻\n",
      "   10 😀😊😗😒😚😄😈😍😉😓😻\n",
      "   7 😀😆😽😐😫😄🙢😈😍😉😓😻\n",
      "   6 😀😊😼😛🙡😊😗😒😚😄🙢😈😍😉😓😻\n",
      "   5 😀😆😴😐🙍😄🙢😈😍😉😓😻\n",
      "   5 😀😆😴😐😫😄🙢😈😍😉😓😻\n",
      "   4 😀😆🙤🙄😽😐🙍😄😈😍😉😓😻\n",
      "   4 😀😆🙄😽😐🙍😄🙢😈😍😉😓😻\n",
      "   4 😀😆🙄😽😐🙍😄😈😍😉😓😻\n",
      "   4 😀😆😐😫😄🙢😈😍😉😓😻\n",
      "\n",
      "😤\n",
      "   8 😀😊😗😒😚😄😈😍😉😓🙖😳😣😤\n",
      "   7 😀😊😗😒😚😄🙢😈😍😉😓🙖😳😣😤\n",
      "   5 😀😊😗😒😚😄😈😍😉😓😣😤\n",
      "   5 😀😆🙄😽😐🙍😄🙢😈😍😉😓🙖😳😣😤\n",
      "   4 😀😆🙄😽😐😫😄😈😍😉😓🙖😳😣😤\n",
      "   3 😀😊😼😛🙡😊😗😒😚😄🙢😈😍😉😓😣😤\n",
      "   3 😀😆🙤🙄😽😐🙍😄🙢😈😍😉😓😣😤\n",
      "   3 😀😆🙤🙄😽😐🙍😄😈😍😉😓🙖😳😣😤\n",
      "   3 😀😆🙇🙄😽😐😫😄😈😍😉😓😣😤\n",
      "   3 😀😆😐🙍😄🙢😈😍😉😓🙖😳😣😤\n",
      "\n",
      "😇\n",
      "   1 😀😶🙋😬😱🙌😧😭🙓😅😄😏😂😸😂🙑😸😂😌😯😌😂🙑😸😂🙑😸😂🙝🙣🙜🙀😂😌😂😈😠😌😯😌😂😯😌😯😌😯😌😯😌😯😌😂😯😌😯😌😯😌😯😌😂😈😵😰😌😯😌😂😸😂😸😎😁🙕😑😇😋😜😃😪😦😹😁🙕😑😇😋😜😃😪😦😁😑😇😋😇😋😇😋😇\n",
      "   1 😀😬😱🙔🙐🙌😧😭🙓😅😄😕😀😬😱🙌😧😭🙓😅😢😩🙘😄😈🙉😂🙑😸😎🙈😁😑😇😋😜😃😪😦😁😑😇😋😇😋😇😋😜😃😦😹😁🙕😑😇😋😇😋😇😋😜😃😦😁🙕😑😇😋😇😋😜😃😪😦😹😁😑😇😋😜😃😪😦😹😁🙕😑😇😋😜😃😪😦😹😁🙕😑😇😋😇\n",
      "   1 😀😊😼😛🙡😊😗😒😚😄😈😠😌😯😌😂😯😌😯😌😂🙝🙜🙀😂😌😂😈🙉😂😈😂😯😌😂😌😯😌😯😌😂😌😂🙑😸😂😌😯😌😯😌😯😌😂😸😂😸😂😈🙉😂😸😂😯😌😂🙑😸😂😯😌😯😌😂🙝🙣🙜🙀😂😯😌😂😈😵😰😌😯😌😯😌😂🙑😸😎😁🙕😑😇😋😇\n",
      "   1 😀😆🙇🙄😽😐🙍😄🙢😈😌😯😌😂😯😌😯😌😂🙝🙜🙀😂😯😌😯😌😂😯😌😂😌😯😌😂😯😌😯😌😯😌😯😌😂😯😌😂😯😌😯😌😯😌😂🙝🙜🙀😂😈🙉😂🙝🙣🙜🙀😂😯😌😂🙝🙣🙜🙀😂😸😂😯😌😂😯😌😂🙝🙜🙀😂🙑😸😂🙝🙣🙜🙀😂🙑😸😎😁😑😇\n",
      "   1 😀😆😐😫😄🙢😈😵😰😌😯😌😂🙑😸😂😌😯😌😂🙝🙜🙀😂😯😌😯😌😂😌😂🙝🙜🙀😂🙝🙣🙜🙀😂😯😌😂😯😌😂😈😌😯😌😂😈😂😌😂🙑😸😂😈🙉😂🙝🙣🙜🙀😂🙑😸😂😈😵😰😌😯😌😂😈😌😯😌😯😌😯😌😂🙑😸😂😈😌😯😌😯😌😂😸😎😁😑😇\n",
      "   1 😀😆😐😙😐😙😐😫😄😈😂🙝🙣🙜🙀😂😈😌😯😌😂🙝🙜🙀😂🙝🙜🙀😂🙝🙣🙜🙀😂😈😵😰😌😯😌😯😌😂😈🙉😂🙝🙜🙀😂😯😌😂😈😂😌😂🙝🙣🙜🙀😂😌😂😈😠😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😌😂😌😯😌😂😸😎🙈😁😑😇😋😇\n",
      "\n",
      "🙕\n",
      "   1 😀😬😱😧🙌😧🙌😧🙌😧🙌😧🙌😧😭🙓😅😢😄😺😏😂🙑😸😂😯😌😂😈😵😰😌😂😈😌😯😌😂😌😂😯😌😂😌😂🙑😸😂🙑😸😎😁🙕😑😇😋😜😃😪😦😁😑😇😋😜😃😦😹😁😑😇😋😇😋😜😃😦😁😑😇😋😇😋😜😃😪😦😹😁😑😇😋😇😋😜😃😪😦😁🙕\n",
      "   1 😀😆😽😐😫😄😺😄😏😂😌😯😌😯😌😯😌😯😌😂😌😯😌😯😌😯😌😂😯😌😯😌😂🙝🙣🙜🙀😂🙑😸😂😈😠😌😯😌😯😌😯😌😂😌😯😌😂🙝🙜🙀😂🙑😸😂🙝🙜🙀😂🙝🙜🙀😂😯😌😯😌😂😸😂😯😌😂🙑😸😂🙑😸😎🙈😁🙕😑😇😋😇😋😜😃😦😹😁🙕\n",
      "\n",
      "😁\n",
      "   1 😀😶🙋😬😱🙔🙐🙌😧😭🙓😅😄😞😕😀😶🙋😬😱🙔🙐🙌😧🙌😧🙌😧🙌😧🙌😧😭🙓😅😄😩🙘😄😈😌😂🙝🙜🙀😂🙝🙣🙜🙀😂😸😎😁🙕😑😇😋😜😃😦😁😑😇😋😜😃😦😁🙕😑😇😋😇😋😜😃😦😁😑😇😋😇😋😜😃😦😁😑😇😋😇😋😇😋😜😃😦😹😁\n",
      "   1 😀😶🙋😬😱😧😭😅😄😞😕😀😊😗😒😚😄🙢😈😌😯😌😯😌😂😈😠😌😂🙑😸😂😌😯😌😂🙝🙣🙜🙀😂😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂😈😠😌😂😸😂🙑😸😂😯😌😂🙝🙣🙜🙀😂😯😌😂😯😌😯😌😂🙝🙜🙀😂😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂😸😎🙈😁\n",
      "   1 😀😶😬😱🙔🙐🙌😧😭😅😢😩😄😞😥🙒😕😀😬😱🙌😧😭🙓😅😄😀😊😗😒😚😄😈😌😂😈😌😯😌😯😌😂😯😌😯😌😯😌😂😸😂😈😌😂😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😯😌😯😌😂🙝🙣🙜🙀😂🙑😸😂😯😌😂🙑😸😎😁😑😇😋😜😃😦😹😁\n",
      "   1 😀😶😬😱😧🙌😧🙌😧😭🙓😅😢😩😄😺😏😂😌😂😯😌😂😌😂😯😌😯😌😯😌😂🙝🙜🙀😂😯😌😂🙝🙣🙜🙀😂🙑😸😂😸😎😁😑😇😋😜😃😪😦😁🙕😑😇😋😜😃😦😁🙕😑😇😋😇😋😇😋😜😃😦😹😁😑😇😋😇😋😜😃😪😦😁🙕😑😇😋😇😋😜😃😦😹😁\n",
      "   1 😀😶😬😱😧😭🙓😅😄😅😄😈🙉😂😈😵😰😌😯😌😂🙝🙣🙜🙀😂🙑😸😂🙝🙣🙜🙀😂😈😠😌😂😈😵😰😌😂😈😵😰😌😂😸😎😁🙕😑😇😋😜😃😪😦😹😁🙕😑😇😋😇😋😇😋😜😃😦😹😁😑😇😋😜😃😪😦😁🙕😑😇😋😜😃😪😦😁🙕😑😇😋😜😃😪😦😁\n",
      "   1 😀😬😱🙌😧😭😅😢🙂🙅🙚😩😄😩🙘😄😞😕😀😆😴😐🙍😄😀😬😱🙌😧🙌😧😭🙓😅😢😩😄😺😄😈😵😰😌😯😌😯😌😂🙝🙣🙜🙀😂😌😯😌😯😌😂😯😌😯😌😂🙝🙣🙜🙀😂😌😯😌😯😌😂😸😂😸😎🙈😁😑😇😋😇😋😜😃😪😦😁😑😇😋😇😋😜😃😦😁\n",
      "   1 😀😬😱🙌😧😭😅😄😀😊🙃😛🙡😊😗😒😚😄😈😵😰😌😯😌😂😌😯😌😯😌😯😌😯😌😯😌😂😌😯😌😯😌😯😌😂😯😌😯😌😂😯😌😯😌😂😯😌😯😌😯😌😯😌😂😯😌😂😌😂😸😎🙈😁🙕😑😇😋😜😃😪😦😁🙕😑😇😋😜😃😦😁🙕😑😇😋😜😃😪😦😹😁\n",
      "   1 😀😬😱😧🙌😧😭🙓😅😢😩😄😩😄🙢😈😌😯😌😂😌😂😸😂😌😯😌😯😌😯😌😯😌😂😌😯😌😂😌😯😌😯😌😯😌😯😌😂😌😯😌😯😌😂😯😌😂😯😌😯😌😯😌😂😯😌😯😌😂😸😂😈🙉😂😌😯😌😂😸😎🙈😁😑😇😋😜😃😦😁🙕😑😇😋😇😋😜😃😦😹😁\n",
      "   1 😀😬😱😧😭🙓😅😢😄😞😕😀😊😗😒😚😄😏🙊😝😂🙑😸😂😌😂😯😌😯😌😂😈😍😉😓😂😈😵😰😌😂😌😂😯😌😂😌😯😌😂😈😌😂😯😌😂😸😂😌😯😌😂😯😌😯😌😂😯😌😯😌😂😌😂🙑😸😎🙈😁😑😇😋😇😋😜😃😪😦😁🙕😑😇😋😇😋😜😃😦😹😁\n",
      "   1 😀😊😼😛🙡😊😗😒😚😄😈😠😌😯😌😯😌😯😌😯😌😂😸😂🙑😸😂😸😂😯😌😂😌😂🙑😸😂😌😯😌😯😌😂😸😂😯😌😂🙝🙣🙜🙀😂😈🙉😂🙝🙜🙀😂😈😵😰😌😯😌😯😌😯😌😂🙝🙜🙀😂😈😵😰😌😯😌😯😌😯😌😯😌😯😌😯😌😂😈🙉😂🙑😸😎🙈😁\n",
      "\n",
      "😂\n",
      "   1 😀😶🙋😬😱🙔🙐🙌😧😭🙓😅😢🙂🙅😩🙘😄😩🙘😄😅😄😩😄😏😂😯😌😂😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😈🙉😂😈😌😯😌😯😌😂😌😯😌😂😯😌😯😌😂😯😌😂😈😍😉😓😣😂🙝🙜🙀😂🙝🙜🙀😂😈🙉😂😯😌😯😌😯😌😂😌😯😌😂\n",
      "   1 😀😶😬😱🙔🙐🙌😧😭🙓😅😄😺😄😏🙊😝😂🙝🙜🙀😂😯😌😯😌😯😌😂😈😍😉😓🙖😳😣😂😈😠😌😯😌😯😌😂😯😌😯😌😂🙑😸😂🙝🙣🙜🙀😂🙑😸😂😈😂🙝🙜🙀😂🙑😸😂😸😂🙝🙜🙀😂😌😂🙝🙜🙀😂🙝🙜🙀😂😈😠😌😯😌😂🙝🙜🙀😂😌😯😌😂\n",
      "   1 😀😬😱🙌😧🙌😧🙌😧🙌😧🙌😧😭😅😄😏😂😈😂😈😠😌😯😌😂😌😯😌😯😌😂🙑😸😂😸😂🙑😸😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂🙑😸😂🙝🙜🙀😂😈🙉😂😈😌😂😈😠😌😂🙑😸😂🙝🙣🙜🙀😂🙝🙜🙀😂😯😌😯😌😂😌😯😌😂🙝🙜🙀😂😈😍😉😓🙖😳😣😂\n",
      "   1 😀😊😼😛🙡😊😗😒😚😄😏🙊😏🙊😝😂😯😌😯😌😯😌😯😌😯😌😂😈😌😯😌😯😌😂😈🙉😂😌😂😌😯😌😂😯😌😂😈😠😌😂😯😌😂🙝🙣🙜🙀😂😯😌😯😌😂😈😵😰😌😂😸😂😯😌😯😌😯😌😯😌😯😌😯😌😂😯😌😂😌😂😯😌😯😌😂🙝🙣🙜🙀😂😸😂\n",
      "   1 😀😆🙤🙄😽😐🙍😄🙢😈😠😌😂🙝🙣🙜🙀😂🙝🙜🙀😂😯😌😯😌😯😌😯😌😯😌😂😯😌😯😌😯😌😯😌😂🙑😸😂😌😯😌😂😯😌😯😌😂🙝🙣🙜🙀😂🙝🙜🙀😂🙑😸😂😸😂🙑😸😂🙑😸😂😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂😌😂😌😯😌😯😌😂😈😌😯😌😂\n",
      "   1 😀😆🙤🙄😽😐😙😐😙😐😙😐😙😐😫😄😩🙘😄😞😥😕😀😶😬😱🙌😧😭🙓😅😄😕😀😊😗😒😚😄🙢😈😍😉😓🙖😳😣😂😌😯😌😯😌😯😌😂😸😂😈😠😌😂🙝🙜🙀😂😈🙉😂😌😂😌😯😌😂😈🙉😂🙝🙣🙜🙀😂😈🙉😂😯😌😂🙑😸😂😈😠😌😯😌😂😸😂\n",
      "   1 😀😆🙄😽😐😫😄😞😥🙒😕😀😊😗😒😚😄😀😬😱🙌😧🙌😧😭🙓😅😄😏😂🙑😸😂😈😌😂😯😌😂🙝🙣🙜🙀😂😯😌😯😌😂😯😌😯😌😯😌😯😌😯😌😂😌😂😌😂😈😵😰😌😯😌😂🙝🙜🙀😂😌😂😸😂😌😂😈😠😌😂😸😂😌😯😌😯😌😂😯😌😯😌😯😌😂\n",
      "   1 😀😆😽😐😙😐🙍😄😏🙊😏🙊😏🙊😝😂😯😌😂😌😂🙝🙜🙀😂😯😌😂😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂🙑😸😂😌😯😌😂😈😌😂😯😌😂😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂😯😌😂🙑😸😂😯😌😂🙑😸😂🙝🙣🙜🙀😂😈😠😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂\n",
      "\n",
      "😋\n",
      "   1 😀😶🙋😬😱🙔🙐🙌😧😭🙓😅😄😞😥😕😀😶🙋😬😱🙌😧🙌😧😭😅😢😩😄😩🙘😄😈😠😌😯😌😂😌😯😌😂😯😌😂😸😂🙝🙜🙀😂🙝🙣🙜🙀😂🙝🙜🙀😂😯😌😂😯😌😂🙑😸😂🙝🙣🙜🙀😂😌😂🙝🙣🙜🙀😂🙝🙜🙀😂😌😯😌😂😯😌😂🙑😸😎😁😑😇😋\n",
      "   1 😀😬😱😧😭🙓😅😢😄😀😬😱🙔🙐🙌😧😭🙓😅😢😄🙢😈😂😌😂😌😂😯😌😯😌😂😈🙉😂😈😠😌😯😌😯😌😯😌😂😌😂🙝🙜🙀😂😸😂😌😯😌😂😸😎🙈😁😑😇😋😇😋😇😋😜😃😪😦😹😁🙕😑😇😋😜😃😦😁😑😇😋😇😋😇😋😇😋😜😃😦😹😁😑😇😋\n",
      "   1 😀😆🙄😽😐🙍😄😕😀😆🙤🙄😽😐🙍😄😀😆🙄😽😐🙍😄😺😄😅😢😩😄😩🙘😄😀😆😴😐🙍😄😅😢🙂🙅🙚😩😄😩😄😀😆😴😐😫😄😈😂😸😂😯😌😯😌😂🙝🙜🙀😂😈😠😌😯😌😂🙑😸😂😈😂😯😌😂🙑😸😎🙈😁😑😇😋😜😃😪😦😹😁🙕😑😇😋😇😋\n",
      "\n",
      "😌\n",
      "   1 😀😶🙋😬😱🙌😧😭😅😢🙂🙅🙚😩🙘😄🙢😈😂😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😯😌😯😌😯😌😯😌😂😯😌😯😌😂😯😌😯😌😯😌😯😌😂😈😠😌😂😯😌😂😌😯😌😯😌😂😈😵😰😌😯😌😯😌😯😌😯😌😂😯😌😯😌😂🙑😸😂😈🙉😂🙝🙣🙜🙀😂😌\n",
      "   1 😀😶😬😱🙔🙐🙌😧😭🙓😅😄😕😀😶😬😱🙌😧😭🙓😅😢😩😄😅😢🙂🙅🙚😩🙘😄🙢😈😠😌😂😯😌😂😌😯😌😯😌😂😈😌😯😌😯😌😂😈😌😂😯😌😂😸😂😈😌😂🙝🙣🙜🙀😂🙝🙜🙀😂🙝🙣🙜🙀😂😈🙉😂😸😂🙝🙜🙀😂😸😂🙝🙜🙀😂😸😂😯😌😯😌\n",
      "   1 😀😶😬😱🙌😧🙌😧😭🙓😅😄😞😥😕😀😆😴😐😫😄🙢😈😵😰😌😂🙝🙣🙜🙀😂🙑😸😂😌😯😌😂😸😂🙝🙜🙀😂😯😌😯😌😂🙝🙜🙀😂😈😵😰😌😯😌😯😌😯😌😯😌😯😌😯😌😂😯😌😂😯😌😂🙝🙜🙀😂😌😯😌😂😌😂😈😠😌😯😌😯😌😯😌😂😌😯😌\n",
      "   1 😀😬😱🙌😧🙌😧😭🙓😅😢😩🙘😄😈😵😰😌😂😯😌😯😌😂😌😯😌😂🙝🙣🙜🙀😂😈😠😌😯😌😂😯😌😯😌😯😌😯😌😯😌😯😌😂😯😌😯😌😯😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂😸😂🙝🙜🙀😂🙝🙣🙜🙀😂😯😌😯😌😯😌😂😸😂🙑😸😂🙝🙣🙜🙀😂😌😯😌\n",
      "   1 😀😆🙤🙄😽😐😫😄😞😕😀😶🙋😬😱😧🙌😧😭😅😄😏🙊😏🙊😏🙊😝🙏🙀😂😯😌😂😯😌😂🙝🙣🙜🙀😂😯😌😯😌😯😌😂😌😯😌😂🙑😸😂😯😌😂😯😌😂😌😯😌😂🙑😸😂🙝🙣🙜🙀😂😈😍😉😓😂🙝🙣🙜🙀😂😌😂😈😵😰😌😯😌😯😌😯😌😂😌😯😌\n",
      "\n",
      "🙀\n",
      "   1 😀😆😽😐😙😐🙍😄😀😊😼😛🙡😊😗😒😚😄😅😢🙂🙅🙚😩🙘😄😏🙊😏🙊😝🙏🙀😂😈😂😌😯😌😯😌😯😌😂🙝🙜🙀😂🙑😸😂😯😌😯😌😂😯😌😯😌😂😸😂🙝🙜🙀😂😌😂😌😂😈😵😰😌😂😌😂🙝🙜🙀😂🙝🙣🙜🙀😂😈😌😂😯😌😯😌😂😸😂🙝🙜🙀\n",
      "\n",
      "😈\n",
      "   1 😀😆🙤🙄😽😐😫😄😩😄😞😕😀😊😼😛🙡😊😗😒😚😄🙢😈😂🙝🙣🙜🙀😂😯😌😂😸😂🙑😸😂😈🙉😂🙝🙜🙀😂🙝🙜🙀😂😌😯😌😯😌😂😸😂🙝🙜🙀😂😯😌😯😌😯😌😂😸😂😌😂😌😂😯😌😯😌😂😌😯😌😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂🙑😸😂😈\n",
      "\n",
      "😃\n",
      "   1 😀😬😱🙔🙐🙌😧🙌😧🙌😧😭🙓😅😢😄😩😄😀😊😗😒😚😄😈😌😂😈😠😌😂😯😌😯😌😯😌😯😌😂😈😂😯😌😯😌😯😌😂😌😯😌😯😌😂😈🙉😂😈😂😌😂🙝🙜🙀😂🙑😸😂😌😂😯😌😂😈😠😌😂😸😎😁🙕😑😇😋😜😃😪😦😹😁😑😇😋😇😋😇😋😜😃\n",
      "   1 😀😬😱🙔🙐🙌😧🙌😧😭😅😄😀😶😬😱😧😭🙓😅😢😩😄😅😢🙂🙅🙚😩😄😏😂😸😂🙑😸😎🙈😁😑😇😋😜😃😪😦😹😁😑😇😋😇😋😜😃😦😹😁😑😇😋😇😋😇😋😜😃😦😁😑😇😋😇😋😇😋😇😋😜😃😪😦😹😁😑😇😋😜😃😪😦😹😁😑😇😋😇😋😜😃\n",
      "   1 😀😬😱🙔🙐🙌😧😭🙓😅😄😺😏🙊😏🙊😝🙏🙀😂😯😌😯😌😯😌😯😌😯😌😂😸😂😈😠😌😯😌😂😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😌😯😌😂🙑😸😂😈🙉😂😈😍😉😓😣😂🙑😸😎😁😑😇😋😜😃😦😹😁😑😇😋😜😃😦😹😁😑😇😋😜😃\n",
      "   1 😀😬😱😧😭🙓😅😢😄😀😶😬😱🙔🙐🙌😧🙌😧🙌😧🙌😧🙌😧🙌😧😭😅😢😄😅😄😺😄😕😀😶🙋😬😱🙌😧🙌😧😭😅😢😄🙢😈😠😌😯😌😂😯😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂😸😂😸😎🙈😁😑😇😋😜😃😪😦😹😁😑😇😋😇😋😜😃😪😦😹😁😑😇😋😜😃\n",
      "   1 😀😊😗😒😚😄😏🙊😝🙏🙀😂😸😂🙑😸😂😈😂😯😌😂🙑😸😂😯😌😂🙑😸😂😌😂😈😂😌😯😌😯😌😂🙑😸😂😯😌😯😌😯😌😯😌😯😌😂😸😂😯😌😯😌😯😌😯😌😂😸😎😁🙕😑😇😋😇😋😜😃😦😁😑😇😋😜😃😦😹😁😑😇😋😜😃😦😹😁😑😇😋😜😃\n",
      "   1 😀😆🙤🙄😽😐🙍😄😞😕😀😊😗😒😚😄😈😵😰😌😂😯😌😂😌😯😌😂😯😌😯😌😯😌😂😈🙉😂😌😯😌😂😸😂🙝🙜🙀😂😸😂🙝🙜🙀😂😸😂😸😎😁🙕😑😇😋😇😋😇😋😇😋😇😋😇😋😜😃😦😹😁😑😇😋😜😃😦😁😑😇😋😜😃😦😹😁😑😇😋😇😋😜😃\n",
      "   1 😀😆🙇🙄😽😐😙😐🙍😄😈😌😯😌😯😌😂😌😯😌😯😌😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂😯😌😯😌😂🙝🙣🙜🙀😂🙑😸😎🙈😁🙕😑😇😋😇😋😇😋😜😃😦😹😁😑😇😋😇😋😜😃😦😁😑😇😋😇😋😜😃😪😦😹😁🙕😑😇😋😇😋😜😃😪😦😁😑😇😋😜😃\n",
      "\n",
      "😑\n",
      "   1 😀😶🙋😬😱🙌😧🙌😧😭🙓😅😄😩🙘😄😏😂😯😌😂🙝🙣🙜🙀😂😸😂🙝🙣🙜🙀😂🙝🙜🙀😂😯😌😂🙝🙣🙜🙀😂😯😌😂🙑😸😂🙑😸😎🙈😁😑😇😋😜😃😪😦😁😑😇😋😇😋😜😃😦😁😑😇😋😇😋😜😃😦😁🙕😑😇😋😜😃😪😦😹😁😑😇😋😜😃😦😁😑\n",
      "   1 😀😶🙋😬😱🙌😧😭😅😢😩🙘😄🙢😈😌😯😌😯😌😂🙝🙣🙜🙀😂😈😌😂😯😌😯😌😂😯😌😯😌😯😌😂😯😌😯😌😯😌😯😌😯😌😯😌😂😯😌😂😌😂🙝🙣🙜🙀😂😌😂😌😂😌😂😯😌😯😌😯😌😯😌😯😌😯😌😂😈😍😉😓😂😈😌😂😯😌😂😌😂😸😎😁😑\n",
      "   1 😀😬😱😧😭🙓😅😢😩🙘😄🙢😈🙉😂😯😌😂😌😯😌😂😯😌😂😈😌😯😌😯😌😂😯😌😂🙝🙜🙀😂🙝🙜🙀😂😌😯😌😯😌😂🙑😸😂😸😂😈😠😌😂🙝🙣🙜🙀😂😈😌😂😈😂😯😌😂😈😠😌😯😌😂😌😂😸😂🙑😸😂🙑😸😎🙈😁😑😇😋😜😃😪😦😹😁🙕😑\n",
      "   1 😀😊😗😒😚😄😀😊😼😛🙡😊😗😒😚😄😺😏🙊😝😂🙝🙜🙀😂😯😌😯😌😯😌😂😈😠😌😯😌😂🙑😸😂😯😌😯😌😂😯😌😯😌😂😸😂😈😌😯😌😂🙝🙣🙜🙀😂😯😌😯😌😂🙑😸😎😁😑😇😋😜😃😪😦😹😁😑😇😋😜😃😦😁🙕😑😇😋😜😃😪😦😹😁🙕😑\n",
      "\n",
      "😹\n",
      "   1 😀😊😼😛🙡😊😗😒😚😄😺😄😕😀😆🙇🙄😽😐😫😄😺😏😂😯😌😂🙑😸😂😈😠😌😂😸😂😸😂😯😌😂🙝🙜🙀😂😌😂😯😌😯😌😯😌😯😌😂😌😯😌😂🙝🙜🙀😂😌😯😌😯😌😯😌😯😌😯😌😂😸😎🙈😁🙕😑😇😋😇😋😇😋😜😃😦😹😁😑😇😋😜😃😦😹\n",
      "\n",
      "😉\n",
      "   1 😀😊😗😒😚😄😏😂😯😌😯😌😂😯😌😯😌😯😌😂🙝🙜🙀😂😌😂😈😌😯😌😯😌😯😌😯😌😯😌😂🙑😸😂😈😂😈😂😈😂🙝🙜🙀😂😯😌😂😯😌😯😌😯😌😂😈🙉😂😸😂😌😯😌😂😸😂🙝🙜🙀😂😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😈🙉😂😈😂😈😍😉\n",
      "\n",
      "🙜\n",
      "   1 😀😆😐🙍😄😺😄😕😀😆😽😐😫😄😞😥😕😀😊😗😒😚😄😞😕😀😬😱🙌😧😭🙓😅😢🙂🙅🙚😩🙘😄😞😕😀😬😱🙔🙐🙌😧😭🙓😅😢🙂🙅🙚😩🙘😄😏🙊😝😂😯😌😂😯😌😯😌😯😌😂🙑😸😂😯😌😂😈😂🙝🙣🙜🙀😂😈🙉😂😈😵😰😌😂🙝🙜🙀😂🙝🙜\n",
      "\n",
      "🙝\n",
      "   1 😀😬😱🙌😧😭🙓😅😢😄😏🙊😝😂😸😂😯😌😯😌😂😈😵😰😌😂🙑😸😂😌😯😌😂😌😂😈😌😂😌😂😈😂😈😠😌😯😌😯😌😂🙑😸😂😯😌😂😸😂🙑😸😂😯😌😯😌😯😌😂😈😵😰😌😯😌😂😸😂😸😂😯😌😯😌😯😌😂😯😌😯😌😂😯😌😂🙝🙣🙜🙀😂🙝\n",
      "   1 😀😆🙤🙄😽😐😙😐😫😄😈😌😂😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂🙝🙜🙀😂😯😌😂😯😌😯😌😯😌😂😸😂😌😯😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂🙑😸😂😯😌😯😌😂😸😂😈😂😯😌😯😌😯😌😂😌😂🙝🙜🙀😂😯😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂🙝\n",
      "\n",
      "😯\n",
      "   1 😀😶😬😱🙔🙐🙌😧😭🙓😅😢😄😅😢😩🙘😄🙢😈😍😉😓🙖😳😣😂😈🙉😂😌😯😌😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂😯😌😯😌😂😈😠😌😂😯😌😂😌😂😌😯😌😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂🙝🙜🙀😂😯😌😂😌😂😌😯😌😂😈😌😯😌😯😌😂😌😯\n",
      "   1 😀😊🙃😛🙡😊😼😛🙡😊😼😛🙡😊😗😒😚😄😏🙊😏🙊😏🙊😏😂😈😠😌😂🙝🙜🙀😂😌😂😯😌😂🙝🙣🙜🙀😂😈😵😰😌😂😸😂😯😌😯😌😂😈😌😂😯😌😯😌😂🙑😸😂😈😠😌😂🙝🙜🙀😂😸😂😯😌😂😌😯😌😯😌😯😌😯😌😂🙝🙜🙀😂😯😌😯😌😂😯\n",
      "   1 😀😊😗😒😚😄😅😄😕😀😆😴😐😫😄😩🙘😄😺😏😂😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂😈😵😰😌😯😌😯😌😯😌😯😌😯😌😂😌😯😌😂🙑😸😂😸😂😯😌😯😌😯😌😂🙝🙜🙀😂😸😂😸😂😯😌😂😯😌😯😌😯😌😂🙝🙜🙀😂😯😌😂🙑😸😂😯😌😯😌😂😌😯\n",
      "\n",
      "😦\n",
      "   1 😀😶😬😱🙌😧😭😅😄😈😠😌😂😯😌😯😌😯😌😯😌😂🙝🙣🙜🙀😂😌😯😌😂😯😌😯😌😯😌😂😈😂😌😂🙝🙜🙀😂🙝🙜🙀😂🙑😸😂🙑😸😎🙈😁😑😇😋😜😃😪😦😹😁😑😇😋😇😋😇😋😜😃😪😦😹😁😑😇😋😇😋😇😋😜😃😪😦😹😁🙕😑😇😋😜😃😦\n",
      "   1 😀😆🙇🙄😽😐🙍😄😅😢😄😏😂😌😯😌😯😌😯😌😯😌😂😌😂😯😌😯😌😯😌😂😯😌😂😸😂😯😌😂😌😂😌😂😌😯😌😂😯😌😯😌😯😌😯😌😯😌😂🙝🙜🙀😂🙑😸😂😯😌😯😌😯😌😯😌😯😌😂🙑😸😎🙈😁😑😇😋😇😋😇😋😜😃😦😁🙕😑😇😋😜😃😦\n",
      "   1 😀😆😐😙😐🙍😄🙢😈😂😯😌😯😌😯😌😯😌😯😌😯😌😂🙝🙜🙀😂😈😌😯😌😯😌😂😌😂😯😌😂😌😯😌😯😌😂🙝🙜🙀😂😯😌😯😌😯😌😯😌😯😌😂😯😌😯😌😯😌😯😌😂😯😌😯😌😂😯😌😂😈🙉😂🙑😸😎🙈😁😑😇😋😜😃😦😹😁🙕😑😇😋😜😃😦\n",
      "\n",
      "🙑\n",
      "   1 😀😶😬😱🙌😧😭😅😄😀😆😴😐😙😐🙍😄😀😬😱😧😭🙓😅😢😄😅😄😩😄😈😌😯😌😂😈🙉😂😸😂🙑😸😂🙝🙣🙜🙀😂😈🙉😂🙝🙜🙀😂🙑😸😂🙑😸😂🙑😸😂😌😯😌😂🙑😸😂😸😂😌😯😌😂😸😂😸😂😯😌😂🙝🙣🙜🙀😂😯😌😂😯😌😯😌😯😌😂🙑\n",
      "   1 😀😊😗😒😚😄😀😬😱🙌😧🙌😧😭🙓😅😢😄😩😄😈😌😯😌😯😌😯😌😯😌😯😌😯😌😯😌😂😌😂😈😌😂😈😠😌😂😌😂😌😯😌😯😌😯😌😂😯😌😂😸😂🙑😸😂😌😂😌😯😌😯😌😂😈🙉😂😈😂😌😯😌😯😌😯😌😯😌😂😌😯😌😯😌😂😌😯😌😯😌😂🙑\n",
      "\n",
      "😜\n",
      "   1 😀😊😗😒😚😄😅😄😺😏🙊😝🙏🙀😂🙝🙣🙜🙀😂🙑😸😂🙑😸😂😯😌😯😌😯😌😂😸😂😯😌😯😌😯😌😂🙑😸😂🙝🙣🙜🙀😂🙑😸😂😸😎🙈😁😑😇😋😇😋😜😃😪😦😹😁🙕😑😇😋😜😃😦😹😁😑😇😋😜😃😪😦😹😁😑😇😋😜😃😪😦😁😑😇😋😇😋😜\n",
      "\n",
      "😸\n",
      "   1 😀😬😱🙌😧😭😅😄😏🙊😝😂😯😌😂😸😂🙑😸😂😈😍😉😓😂😸😂😌😂😯😌😯😌😂😯😌😂😌😯😌😯😌😯😌😯😌😯😌😂🙑😸😂😌😂😌😂😈😵😰😌😯😌😯😌😂🙝🙜🙀😂😌😯😌😂😌😯😌😯😌😯😌😂😌😯😌😂😈😠😌😯😌😯😌😯😌😂🙝🙜🙀😂😸\n",
      "   1 😀😆🙄😽😐🙍😄😺😏😂😌😯😌😯😌😯😌😂😸😂🙑😸😂🙑😸😂😸😂😌😯😌😯😌😂😈😂😈😌😯😌😯😌😂😈😌😂🙝🙣🙜🙀😂🙝🙜🙀😂🙝🙜🙀😂🙝🙣🙜🙀😂😯😌😂🙑😸😂😌😯😌😯😌😯😌😯😌😯😌😂😌😂🙑😸😂😯😌😯😌😯😌😂🙝🙜🙀😂🙑😸\n",
      "   1 😀😆😴😐🙍😄🙢😈😠😌😯😌😯😌😯😌😂🙝🙜🙀😂🙑😸😂😌😯😌😯😌😯😌😂😯😌😂🙝🙣🙜🙀😂🙝🙜🙀😂🙑😸😂🙝🙜🙀😂😯😌😯😌😂🙝🙜🙀😂😸😂🙝🙣🙜🙀😂😈🙉😂😸😂🙝🙣🙜🙀😂😈😂😈😠😌😯😌😂😈😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂🙑😸\n",
      "\n",
      "😪\n",
      "   1 😀😶😬😱🙔🙐🙌😧🙌😧🙌😧🙌😧😭🙓😅😄😀😊😗😒😚😄😞😥😕😀😆🙤🙄😽😐🙍😄😅😢😄😕😀😶🙋😬😱😧😭😅😢🙂🙅😩😄🙢😈😂😸😎🙈😁😑😇😋😜😃😪😦😁😑😇😋😜😃😦😹😁😑😇😋😜😃😪😦😹😁🙕😑😇😋😜😃😪😦😹😁😑😇😋😜😃😪\n",
      "   1 😀😆😽😐🙍😄😕😀😶😬😱😧😭😅😄😕😀😬😱😧🙌😧🙌😧🙌😧😭😅😄😅😄😩🙘😄😕😀😆🙇🙄😽😐🙍😄😏😂😈😌😂🙝🙣🙜🙀😂🙝🙣🙜🙀😂😈😂😌😯😌😯😌😯😌😂😸😎🙈😁😑😇😋😜😃😪😦😹😁🙕😑😇😋😇😋😇😋😜😃😦😹😁😑😇😋😜😃😪\n"
     ]
    }
   ],
   "source": [
    "for term, top in results.items():\n",
    "    print(\"\\n%s\" % term)\n",
    "    tk = top.top()\n",
    "    for c, p in tk:\n",
    "        print(\"   %s %s\" % (c, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
